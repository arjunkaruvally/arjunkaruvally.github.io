<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://arjunkaruvally.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://arjunkaruvally.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-21T19:34:36+00:00</updated><id>https://arjunkaruvally.github.io/feed.xml</id><title type="html">blank</title><subtitle>&quot;Chaos Crafted&quot; </subtitle><entry><title type="html">Fractals</title><link href="https://arjunkaruvally.github.io/blog/2023/fractals/" rel="alternate" type="text/html" title="Fractals"/><published>2023-10-18T00:00:00+00:00</published><updated>2023-10-18T00:00:00+00:00</updated><id>https://arjunkaruvally.github.io/blog/2023/fractals</id><content type="html" xml:base="https://arjunkaruvally.github.io/blog/2023/fractals/"><![CDATA[<p>In my class, “<a href="https://arjunkaruvally.github.io/FYS-NID/">Nature Inspired Design in Computing</a>,” we recently delved into the intriguing world of fractals. I put together this blog post to share some insights from exploring fractals. We’ll start by unraveling the foundational definitions and intuitions, then progress to the mathematics of fractals, highlighting how our standard understanding of spatial dimensions might not quite grasp their unique nature. By the end, we’ll introduce the formal concept of the box counting dimension—often referred to as the <a href="https://en.wikipedia.org/wiki/Minkowski%E2%80%93Bouligand_dimension">Minkowski-Bouligand dimension</a>—illustrating that fractals interestingly possess fractional dimensions!</p> <h2 id="introduction">Introduction</h2> <p><strong>What really are fractals?</strong> Fractals are intricate patterns that repeat themselves, no matter how much you zoom in. The etymology of the term “fractal” (from Latin <em>fractus</em>) is due to this self similar nature, meaning that if you zoom in on a portion of the fractal, you will see a shape that looks similar to the larger scale. This property gives the appearance of being “broken” into smaller parts that look like the whole. Unlike simple shapes like circles or squares, fractals sit somewhere in-between dimensions, making them both fascinating and a bit mysterious. You can spot these patterns everywhere – from the way trees branch out to the designs on seashells.</p> <h3 id="short-history">Short History</h3> <p>Fractals, while seemingly a modern concept, have roots that stretch back far earlier than one might think.</p> <p><strong>Ancient Beginnings</strong>: While the formal study of fractals is relatively recent, many ancient cultures incorporated fractal-like patterns into their art and architecture. The recursive patterns in African art, the infinite loops in Celtic knots, and the intricate designs in Indian mandalas all hint at an innate human appreciation for self-similar patterns.</p> <p><strong>The 19th Century and Early Pioneers</strong>: Mathematicians like Karl Weierstrass, Georg Cantor, and Helge von Koch began exploring functions and sets which didn’t have the properties of classical Euclidean geometry. Koch’s famous ‘snowflake curve’, for instance, was a continuous curve with no tangents – a radical idea for its time.</p> <p><strong>Benoit B. Mandelbrot – The Father of Fractals</strong>: The 20th century saw the birth of fractals as a formal field of study, largely thanks to Benoit B. Mandelbrot. In 1975, he coined the term ‘fractal’ to describe shapes that appear similar at any level of magnification. His work was inspired by earlier mathematicians but also by his study of real-world phenomena, like the irregularity of coastlines. Mandelbrot’s book, “The Fractal Geometry of Nature,” published in 1982, revolutionized how we understand many natural phenomena and patterns.</p> <p><strong>Fractals in the Digital Age</strong>: With the advent of computers, visualizing and generating fractals became much easier. They soon permeated various fields, from computer graphics in movies to the design of antennas and even in financial models. Computers allowed for deep dives into fractal sets, like the famous Mandelbrot set, revealing their endless intricacies.</p> <p>Today, fractals continue to inspire scientists, artists, and thinkers alike. They bridge the gap between abstract mathematics and the tangible patterns we see in the world around us, standing as a testament to the beauty and complexity inherent in nature and mathematics.</p> <h2 id="characteristics-of-a-fractal">Characteristics of a Fractal</h2> <p>Till now, we took fractals as some shape that is somehow differnt from regular shapes like circles, squares, etc. So, if we are provided a shape, how do we see if it is fractal or not? Can we formalize what a fractal really is? It seems not.</p> <p><strong>Attempts to Formally Define Fractals and Their Shortcomings</strong></p> <ol> <li><strong>Self-similarity</strong>: <ul> <li><strong>Definition</strong>: A fractal pattern appears the same (or statistically similar) regardless of the level of magnification.</li> <li><strong>Shortcoming</strong>: Not all fractals show exact self-similarity.</li> <li><strong>Example</strong>: The coastline is often cited as a real-world fractal, due to its rough self-similarity. If you zoom in on a section of the coastline, it can appear jagged and irregular, much like the larger scale. However, the patterns aren’t exactly identical at every scale, making them only <em>statistically</em> self-similar.</li> </ul> </li> <li><strong>Fractional Dimension</strong>: <ul> <li><strong>Definition</strong>: Fractals often have non-integer (or fractional) dimensions.</li> <li><strong>Shortcoming</strong>: Not all objects with fractional dimensions have the visual or structural complexity associated with fractals.</li> <li><strong>Example</strong>: A sparse dusting of sand on a table might have a fractional dimension (because it’s somewhere between a 2D plane and a 3D volume). However, this sand distribution doesn’t exhibit the typical intricate patterns one would associate with fractals.</li> </ul> </li> <li><strong>Recursive Definition</strong>: <ul> <li><strong>Definition</strong>: Fractals can be produced by repeating a simple process.</li> <li><strong>Shortcoming</strong>: Not all fractals are formed through recursion.</li> <li><strong>Example</strong>: The patterns seen in turbulent fluids or the distribution of galaxies in the universe can be fractal-like in nature. Yet, they aren’t formed by a simple, repeated recursive process but rather by more complex interactions and dynamics.</li> </ul> </li> <li><strong>Complexity and Detail</strong>: <ul> <li><strong>Definition</strong>: Fractals are shapes composed of parts similar to the whole, having details observable at every magnification level.</li> <li><strong>Shortcoming</strong>: Using complexity alone is too broad, and many complex patterns might not fit the traditional concept of a fractal.</li> <li><strong>Example</strong>: The branching of veins in a leaf may show intricate patterns with many details, but these patterns might not be self-similar or possess other fractal characteristics. They are complex but might not always be considered fractals based on other criteria.</li> </ul> </li> </ol> <p>I shamelessly take the below section from the Wikipedia to drive home the characteristics of a fractal,</p> <blockquote> <p>One point agreed on is that fractal patterns are characterized by fractal dimensions, but whereas these numbers quantify complexity (i.e., changing detail with changing scale), they neither uniquely describe nor specify details of how to construct particular fractal patterns. In 1975 when Mandelbrot coined the word “fractal”, he did so to denote an object whose Hausdorff–Besicovitch dimension is greater than its topological dimension. However, this requirement is not met by space-filling curves such as the Hilbert curve.</p> <p>Because of the trouble involved in finding one definition for fractals, some argue that fractals should not be strictly defined at all. According to Falconer, fractals should be only generally characterized by a gestalt of the following features;</p> <ul> <li><strong>Self-similarity</strong>, which may include: <ul> <li><em>Exact self-similarity</em>: identical at all scales, such as the Koch snowflake</li> <li><em>Quasi self-similarity</em>: approximates the same pattern at different scales; may contain small copies of the entire fractal in distorted and degenerate forms; e.g., the Mandelbrot set’s satellites are approximations of the entire set, but not exact copies.</li> <li><em>Statistical self-similarity</em>: repeats a pattern stochastically so numerical or statistical measures are preserved across scales; e.g., randomly generated fractals like the well-known example of the coastline of Britain for which one would not expect to find a segment scaled and repeated as neatly as the repeated unit that defines fractals like the Koch snowflake.</li> <li><em>Qualitative self-similarity</em>: as in a time series.</li> </ul> </li> <li> <p><strong>Multifractal scaling</strong>: characterized by more than one fractal dimension or scaling rule Fine or detailed structure at arbitrarily small scales. A consequence of this structure is fractals may have emergent properties (related to the next criterion in this list).</p> </li> <li>Irregularity locally and globally that cannot easily be described in the language of traditional Euclidean geometry other than as the limit of a recursively defined sequence of stages. For images of fractal patterns, this has been expressed by phrases such as “smoothly piling up surfaces” and “swirls upon swirls”;see Common techniques for generating fractals. As a group, these criteria form guidelines for excluding certain cases, such as those that may be self-similar without having other typically fractal features. A straight line, for instance, is self-similar but not fractal because it lacks detail, and is easily described in Euclidean language without a need for recursion.</li> </ul> </blockquote> <p>Even for mathematicians, who are very particular about formally defining concepts, fractals pose a significant challenge. I dont personally have any preferences for a particular definition of a fractal. We don’t have to do that for the purposes of this post.</p> <h2 id="inadequacy-of-conventional-dimensions">Inadequacy of conventional dimensions</h2> <p>As we’ve seen, the journey of fractals has been a rich tapestry of evolving definitions, each trying to encapsulate their mesmerizing complexity. Historically, scientists grappled with the challenge of defining these enigmatic figures, from their self-similarity to their resistance to traditional geometric classification. But why did these definitions matter? Why was it so crucial to understand what exactly a fractal is? The answer lies in the very nature of fractals themselves. By understanding their foundational properties, we can better appreciate their quirks, challenges, and the mysteries they present. And what better way to delve into this understanding than by exploring a practical example?</p> <p>We will consider the Sierpinski triangle to tease out the properties of fractals. By the end of this section, we should be sufficiently convinced that conventional integer dimensional spaces are inadequate to understand them.</p> <h3 id="constructing-a-sierpinski-triangle">Constructing a Sierpinski Triangle</h3> <ol> <li><strong>Draw a Triangle</strong> <ul> <li>Start by drawing an equilateral triangle on a piece of paper. This triangle will be the base for our pattern.</li> </ul> </li> <li><strong>Divide and Hollow Out</strong> <ul> <li>Find the midpoint of each side of the triangle.</li> <li>Connect these midpoints to form a smaller equilateral triangle in the center of the original triangle.</li> <li>Now, “remove” or hollow out this smaller central triangle, leaving you with three smaller equilateral triangles at the corners.</li> </ul> </li> <li><strong>Repeat</strong> <ul> <li>Treat each of the three smaller triangles as if they were the original triangle and repeat the process.</li> <li>Find the midpoints of their sides, connect them, and hollow out the center triangle.</li> <li>This will leave you with 9 even smaller triangles.</li> </ul> </li> <li><strong>Continue Recursively</strong> <ul> <li>Keep repeating the process for each of the small triangles you get at every step. The more times you repeat, the more intricate and “fractal-like” the pattern becomes.</li> </ul> </li> <li><strong>Infinity</strong> <ul> <li>In theory, this process continues infinitely. In practice, you’ll stop when the triangles become too small to draw or discern.</li> </ul> </li> </ol> <p>What we’re left with is a pattern that, no matter how closely you look at it, keeps revealing smaller triangles. self similarity! The Sierpinski triangle may look complicated, but its construction is based on a straightforward and repetitive process. It’s a perfect example of how simple rules can lead to complex and beautiful patterns.</p> <canvas id="full_triangle" width="600" height="600" style="height:60%;width:60%;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p><em>The code for generating this triangle in javascript is in the source code of this webpage. Right-click -&gt; view source -&gt; Search for a function called <code class="language-plaintext highlighter-rouge">tripinski</code></em>.</p> <h3 id="computing-the-perimeter-and-area">Computing the perimeter and area</h3> <p>To understand why introducing a fractional dimension was even necessary for fractals, lets see how much our conventional notions of integer-dimensional spaces work with the Sierpinski triangle. What we mean by “conventional” is that our notion that lines are 1D objects, areas like triangle and rectangle are 2D objects, volumes like sphere and cube are 3D objects and so on. We will use this temporary notion of dimension for now, and formalize it later.</p> <h4 id="perimeter">Perimeter</h4> <p>Computing the perimeter of an infinite object like the Sierpinski triangle is challenging. We can approach this problem is by first figuring our how the perimeter <em>changes</em> when you repeat the process of generating a fractal. Once that is done, we can reason about how the perimeter behaves when approaching infinity.</p> <p>We begin with a full equilateral triangle with area shaded in black. Assume each side has a length of $d$. The perimeter of this triangle is $3d$. Now, we can start our Sierpinski procedure. Let’s create a new variable $P_0 = 3d$ to hold the perimeter at each iteration.</p> <p><strong>Iteration 1</strong></p> <p>Connect the midpoints of the big triangle and remove the resulting middle triangle. The figure we get looks like this</p> <canvas id="triangle_1" width="600" height="600" style="height:60%;width:60%;display:block;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p>Now the perimeter gets added because it has to account for the hole in the middle,</p> \[P_1 = 3d + 3 \frac{d}{2}\] <p>The additional term adds the sides of the new triangle in the middle. This is good, it seems we are starting to find some pattern which is the side length always reduces to half.</p> <p><strong>Iteration 2</strong></p> <p>Connect the midpoints of all the possible sides and remove triangles again. The figure we get looks like this</p> <canvas id="triangle_2" width="600" height="600" style="height:60%;width:60%;display:block;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p>The perimeter now has to account for the new holes we created.</p> \[P_2 = 3d + (3 \frac{d}{2^1}) + 3 (3 \frac{d}{2^2})\] <p>Now, we are starting to see some pattern. It seems at each iteration, the number of triangles grow 3 times and the sidelength of the triangles halves. Let us do one of iteration and see if the pattern holds.</p> <p><strong>Iteration 3</strong></p> <p>Connect the midpoints of all the possible sides and remove triangles again. The figure we get looks like this</p> <canvas id="triangle_3" width="600" height="600" style="height:60%;width:60%;display:block;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p>The perimeter is</p> \[P_4 = 3d + (3 \frac{d}{2^1}) + 3 (3 \frac{d}{2^2}) + 3^2 (3 \frac{d}{2^3})\] \[P_4 = 3d + 3^1 \frac{d}{2^1} + 3^2 \frac{d}{2^2} + 3^3 \frac{d}{2^3}\] <p><strong>Iteration n</strong></p> <p>Now, it is fairly easy to see the pattern. At each iteration, a new term gets added to the area, which is some power of $\frac{3}{2}$ times the sidelength. We can generalize this, and write a form for the perimeter at the $n^{\text{th}}$ iteration.</p> \[P_n = 3d + \sum_{i=1}^n \left( \frac{3}{2} \right)^i d \,\,, n&gt;0\] <p>The second term is a <a href="https://en.wikipedia.org/wiki/Geometric_series">geometric series</a> can be summed over (Check the linked Wikipedia page if a refresher is needed). We can now apply for the formula for the geometric series sum to reduce our perimeter.</p> \[P_n = 2d - d \left(\frac{1-\left(\frac{3}{2}\right)^{n+1}}{\frac{1}{2}}\right)\] <p>What happens to the perimeter when we approach infinity?</p> \[\lim_{n \to \infty} P_n = P_{\infty} = \infty\] <p>So perimeter reaches infinity. It makes sense because we are keeping on adding new triangles and hence new lines contributing to the perimeter.</p> <p>In case, why $P_{\infty}$ is $\infty$ is not clear, we can take a step back. We had $2d-$<em>something</em> and the <em>something</em> is a very high negative number since we have $1-(\frac{3}{2})^{n+1}$ in the numerator and $\frac{3}{2} &gt; 1$ which means it is going to infinity as we power the term.</p> <h4 id="area">Area</h4> <p>Now, lets see what happens to the area. First, lets recap that the area of an equilateral triangle and the entire black triangle is $\frac{\sqrt{3}}{4} d^2$ where $d$ is the side-length. We will call this $A_0$ so that we dont have to keep track of the annoying $\frac{3}{4}$ terms.</p> <p><strong>Iteration 1</strong></p> <canvas id="areaTriangle_1" width="600" height="600" style="height:60%;width:60%;display:block;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p>In iteration 1, we are dividing the big triangle into $3$ small identical triangles. Therefore, the area has to be $A_0/4$ for each of the triangles. Then, we remove one of the triangles. The resultant area is</p> \[A_1 = A_0 - \frac{A_0}{4}\] <p><strong>Iteration 2</strong></p> <canvas id="areaTriangle_2" width="600" height="600" style="height:60%;width:60%;display:block;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p>In iteration 2, we repeat the same process of division for the 3 triangles obtained from the first iteration. Each small triangle will have the area $(A_0/4) \times 1/4 = A_0/4^2$. and we are removing $3$ of these small areas in this iteration.</p> \[A_2 = A_0 - \frac{A_0}{4} - 3 \frac{A_0}{4^2}\] <p><strong>Iteration 3</strong></p> <canvas id="areaTriangle_3" width="600" height="600" style="height:60%;width:60%;display:block;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p>In iteration 3, we repeat the same process of division for the 9 triangles obtained from the second iteration. Each small triangle will have the area $(A_0/4^2) \times 1/4 = A_0/4^3$, and we are removing $3^2$ of these small areas in this iteration.</p> \[A_2 = A_0 - \frac{A_0}{4} - 3 \frac{A_0}{4^2} - 3^2 \frac{A_0}{4^3}\] <p><strong>Iteration n</strong></p> <p>Now, we are in a good place to generalize to $n$ iterations. At each step of the iteration, I am adding a term that is $(3/4)$ times the previous factor. I am doing a bit of algebra (multiply and divide each term by 3) to write the summation in a neat form.</p> \[A_n = A_0 - \sum_{i=1}^n \left( \frac{3}{4} \right)^i \frac{A_0}{3} \,\,\,, n&gt;0\] <p>This is also a geometric series like before. Lets use the summation formula and reduce the summation in the term.</p> \[A_n = \frac{4}{3} A_0 - \left( \frac{1 - \left(\frac{3}{4}\right)^{n+1}}{\frac{1}{4}} \frac{A_0}{3}\right)\] <p>Now, we can figure out what happens when $n$ approaches infinity by applying limits.</p> \[\lim_{n \to \infty} A_n = A_\infty = \frac{4}{3} A_0 - \frac{4}{3} A_0 = 0\] <p>The trick to get this is to see that $(\frac{3}{4})^n$ becomes smaller as the $n$ increases (as $\frac{3}{4} &lt; 1$) with it converging to $0$ when approaching infinity.</p> <h4 id="reflection">Reflection</h4> <p>Now that we have the perimeter and area computed, we can compare the two. Recap:</p> \[P_\infty = \infty \text{ and } A_\infty = 0\] <p>Let’s imagine now iterating through the triangle generation procedure. We take smaller and smaller triangles away from the larger triangle. So, in a way, the “edge” or perimeter of our triangle keeps growing. Now, if we were to paint the inside of this triangle at each iteration, you’d find that there’s less and less space to paint each time. That’s because the area inside the triangle keeps getting smaller.</p> <p>It’s enough to make you scratch your head: a shape with an edge that keeps growing but an inside that’s disappearing!</p> <p>This weirdness is why mathematicians started to think that maybe the usual way we talk about dimensions doesn’t quite work for something like the Sierpinski triangle. I mean, how can we have something that’s bigger on the outside but smaller on the inside?</p> <p>So, this trippy triangle tells us we might need some new math tools to understand these kinds of shapes better.</p> <h2 id="box-counting-dimension">Box-Counting Dimension</h2> <p>Okay, at this point, we know that we need a new notion of dimension, but what exactly is dimension? We will take a look at this question in two ways. First, we will build some intuition about what is actually meant when we say the dimensions of an object. Next, we will introduce the box-counting dimension formally. Feel free to skip the first section, if it seems too basic :)</p> <h3 id="what-is-dimension">What is dimension?</h3> <p>When do we say an object is 1D, 2D, or 3D? Let’s first take a step back and begin with a simple notion of dimension.</p> <blockquote> <p><strong><em>DEFINITION (TEMPORARY #1):</em></strong> Dimension of an object is defined as the minimum number of dimensions in the coordinate space required to describe all the points in the object</p> </blockquote> <p>We know a line is 1D. So, according to our notion of dimension, it must mean that we can represent the line in a 1D space. Now, let’s take a circle of radius 1 without any area, and only the circumference. According to our definition, we need a 2D plane to draw it. But, here is the problem - although we use a 2D plane to represent the circle, you dont really need 2 numbers to describe all of it’s points. That is, you can take any $x$ and solve the equation $y = \sqrt{1-x^2}$ to get y. So, somehow your $x$ uniquely specified all your points in the circle, which must mean that the circle we have is actually a 1D object even through we draw it in 2D coordinate space. Another way to look at this is that you can construct a coordinate system which is not a plane like in the typical coordinate system but is curved along the circumference of the circle such that the circle becomes a 1D object. The point I want to make is the difference between the space in which an object is embedded and the intrinsic dimensionality of the object itself. From the curved vs planar spaces we used for the circle argument, it is clear that we need a new definition for dimensions, one which is independent of the space used to represent the object. Let’s consider the following definition:</p> <blockquote> <p><strong><em>DEFINITION (TEMPORARY #2):</em></strong> Dimension of an object is defined as the minimum number of independent parameters required to specify all the points of the object.</p> </blockquote> <p>This seems to be a nice enough definition for a dimension. We can see now that the minimum number of independent parameters we required for the circle case is actually $1$ (the $x$ value) and the $y = \sqrt{1-x^2}$ is actually dependent on $x$. Another easier way to express the points in the circle is using the angle it makes with one of the coordinate axis ($\theta$). For a hollow circle, all the points are described by this angle and a fixed radius $r$. We can now consider what happens when the circle object of radius 1 encloses an area within its circumference. Now, we need two independent parameters $r$ and $\theta$. The radius parameter $r$ can vary from $0$ to $1$ and the $\theta$ parameter can vary from $0$ to $2 \pi$. Seems like we are on the right track. Now what if I give an arbitrary shape like our Sierpinski fractal? How do we find this minimum number of parameters required to describe it?</p> <p>Let’s see how a simple use of a scale and measuring with it, can get us this number. Suppose we have a scale - a 3D cube with $1 \mu$ side-length using which we can count how many of the scale make up our object which we call measure. We now use this scale and count how many points (in the units of $\mu$) are there in the hollow circle. For a circle of $1 \mu$ radius, the circumference we get will be $c = 2 \pi \mu$. Now, the choice of $\mu$ we made here is arbitrary, we can argue that we don’t like a $\mu$ unit scale anymore, and we want to start using a $\frac{\mu}{2}$ unit scale. How does the circumference we measured change with the new unit? The new circumference is $2 c$ - it doubled!</p> <p>How about a circle which has points within it? In this case, we need to measure the area of the circle to cover all the points. Similar to the above, for $1 \mu$ scale, we get an area of $a = \pi \mu^2$. Now if the scale is $\frac{\mu}{2}$, the area is $4a$. Compared to the hollow circle, our measure of the object has doubled for a filled circle.</p> <p>Let’s take one more step and see if we can establish some trends in the behavior of these measurements we are making. Consider a filled sphere, we need to measure the volume to cover all the points. The volume for the case of a scale of $\mu$ units is $v = \frac{4}{3} \pi \mu^3$ and for the $\frac{\mu}{2}$ scale, the volume becomes $2^3 v$. To summarize, this is how the measures changed when the scale is changed from $\mu$ to $\frac{\mu}{2}$</p> \[1D \text{(circumference)}: 2^{\textcolor{red}{1}} c \text{ and } 2D \text{(area)}: 2^{\textcolor{red}{2}} a \text{ and } 3D \text{(volume)}: 2^{\textcolor{red}{3}} v\] <p>This seems very interesting - somehow the dimensionality of the object is encoded in the exponent of the amount by which the scale is changed (that is as the power of $2$). We can obtain this from the measurement just by applying $\log_2$. Whew! that was a long detour, but we are now in a good position to formalize and find the box counting dimension.</p> <h3 id="formal-definition-for-the-box-counting-dimension">Formal Definition for the Box-Counting dimension</h3> <blockquote> <p><strong>DEFINITION (Box-Counting Dimension)</strong>: Suppose we have a set $S$ defined with a way to measure using some notion of a cube of side $\epsilon&gt;0$ in the set. Let $N_\epsilon$ be the number of $n$-dimensional cubes of side-length $\epsilon$ needed to cover $S$. If there is a number $d$ such that</p> \[d = - \lim_{\epsilon \to 0} \frac{\ln N_\epsilon (S)}{\ln \epsilon}\] <p>The number $d$ is the box counting dimension of the set $S$</p> </blockquote> <p>Intuitively, the box-counting dimension gives us a way to quantify how much “space” an object takes up when we look at it at different scales. The scales here are formalized by taking different sizes ($\epsilon$) for our counting boxes. The dimension is the exponent we found in the previous section (<a href="#what-is-dimension">What is dimension?</a>) that was encoding the dimension of the object.</p> <h3 id="dimension-of-sierpinski-triangle">Dimension of Sierpinski Triangle</h3> <p>Now that we have a very nice definition for dimension, lets apply it to our Sierpinski triangle and see what its dimensions are - hopefully that will shed some light into the counter-intuitiveness of the area, and perimeter calculation. Since Sierpinski triangle is an infinite figure, it is not trivial to just place boxes like in the circle example. Instead, we are going to use the same procedure for computing the area and perimeter - that is figure out the number of boxes at each step in the iteration and figure out what happens when the iteration nears infinity.</p> <p><strong>Iteration 1</strong></p> <canvas id="square_1" width="600" height="600" style="height:60%;width:60%;display:block;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p>We have $3$ boxes here each with sidelength $\frac{d}{2}$. So $N_{\frac{d}{2}} = 3$</p> <p><strong>Iteration 2</strong></p> <canvas id="square_2" width="600" height="600" style="height:60%;width:60%;display:block;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p>We have $9$ boxes here each with sidelengh $\frac{d}{2^2}$. So $N_{\frac{d}{2^2}} = 3^2$</p> <p><strong>Iteration 3</strong></p> <canvas id="square_3" width="600" height="600" style="height:60%;width:60%;display:block;margin-left:auto;margin-right:auto;margin-bottom:10%;"></canvas> <p>We have $27$ boxes here each with sidelength $\frac{d}{2^3}$. So $N_{\frac{d}{2^3}} = 3^3$</p> <p><strong>Iteration i</strong></p> <p>We have some patttern here and let’s generalize the pattern to arbitrary iteration $n$. We get $N_{\frac{d}{2^i}} = 3^{i}$</p> <p><strong>Computing Box-Counting dimension</strong></p> <p>We can now use the formulae we have for the box counting dimension.</p> \[d = - \lim_{\epsilon \to 0} \frac{\ln N_\epsilon (S)}{\ln \epsilon}\] <p>The trick is to use this formulae is to change $\epsilon$ to the iteration $i$. Since $i \to \infty$, the sidelength $\epsilon \to 0$, we can now write the dimension as</p> \[d = - \lim_{i \to \infty} \frac{\ln \left( N_{\frac{d}{2^i}} \right) }{\ln \left( \frac{d}{2^i} \right) }\] <details><summary>Click here for detailed steps</summary> <p>Substituting our $N$ we get,</p> \[d = - \lim_{i \to \infty} \frac{\ln \left( 3^i \right) }{\ln \left( \frac{d}{2^i} \right) }\] \[d = - \lim_{i \to \infty} \frac{i \ln \left( 3 \right) }{\ln \left( d \right) - i \ln \left( 2 \right) }\] <p>We cannot set $i$ to infinity directly here. So, we divide the numerator and denominator by $i$.</p> \[d = - \lim_{i \to \infty} \frac{\ln \left( 3 \right) }{\frac{\ln \left( d \right)}{i} - \ln \left( 2 \right) }\] </details> \[d = \frac{\ln \left( 3 \right) }{\ln \left( 2 \right) } \approx 1.5849\] <p>Earlier, when we examined the Sierpinski triangle, two aspects stood out: its ever-expanding perimeter and its diminishing area with each iteration. At first glance, this appears paradoxical. How can a shape continue to grow outward indefinitely while the space it occupies shrinks? This is where the fractional dimension, as revealed by the box counting method, provides clarity.</p> <p>The Sierpinski triangle’s fractional dimension bridges the gap between the unusual behavior of its perimeter and area. When you consider its boundary, it seems to be constantly stretching out, giving the illusion of infinity. Yet, simultaneously, the shape is riddled with holes, which accounts for the decreasing area. This ‘in-between’ nature of the Sierpinski triangle’s dimension reflects precisely this interplay. It’s as if the triangle is trying to exist in a space between a line and a plane. The perimeter’s endless growth hints towards a dimension greater than 1, while the vanishing area alludes to it being less than a solid 2D figure.</p> <p>Thus, the fractional box counting dimension not only captures the inherent intricacy of the Sierpinski triangle but also elegantly reconciles the peculiarities observed in its area and perimeter. This provides a comprehensive picture of the triangle’s nature, emphasizing the unique space it occupies in the geometric universe.</p> <h2 id="conclusion">Conclusion</h2> <p>We took a dive into the world of fractals and revealed patterns that challenge traditional geometry. By journeying from basic definitions to complex computations, we unveiled the mystery of the fractional dimension using the Sierpinski triangle as our guide.</p> <p>A simple iterative process can create the Sierpinski triangle. Starting with an equilateral triangle and repeatedly removing smaller triangles, we’re left with a pattern that both mesmerizes and puzzles. As the iterations progress, the Sierpinski triangle displays a curious phenomenon: its perimeter seems to grow endlessly, while its area shrinks. This counterintuitive behavior requires a deeper exploration.</p> <p>To understand the Sierpinski triangle’s enigmatic behavior, we introduced the ‘box counting’ method. This approach revealed that the Sierpinski triangle’s dimension isn’t whole but fractional, providing a fresh lens to understand the triangle’s intricate balance between emptiness and substance.</p> <p>The Sierpinski triangle, with its fractional dimension, stands as a testament to the beauty and complexity of fractals. Its unique behavior, bridging the gap between traditional dimensions, offers insights into the rich tapestry of geometry and emphasizes the vastness of the mathematical universe.</p> <script type="text/javascript">function sierpinski_square(n,t,e,i){square(n,t/2,e/2,t/2,e/4,i)}function square(n,t,e,i,a,o){if((o-=1)>0){1==o&&draw_enclosing_square(n,a,i,a+e,i,a+e/2,i+t,"rgba(63,145,201,0.5)");var r=t/2,s=e/2,l=i-r,_=a+s/2,d=i+r,g=a+e-s/2;square(n,r,s,i+r,a-s/2,o),square(n,r,s,l,_,o),square(n,r,s,d,g,o)}}function draw_enclosing_square(n,t,e,i,a,o,r,s){var l=Math.min(t,i,o),_=Math.max(t,i,o),d=Math.min(e,a,r),g=(Math.max(e,a,r),Math.abs(_-l));sq_minX=l-g/2,sq_minY=d,n.beginPath(),n.moveTo(sq_minX,sq_minY),n.lineTo(sq_minX,sq_minY+g),n.lineWidth=10,n.setLineDash([5,15]),n.lineTo(sq_minX+g,sq_minY+g),n.lineTo(sq_minX+g,sq_minY),n.closePath(),n.fillStyle=s,n.fill(),n.strokeStyle="#ff0000",n.stroke(),sq_minX=l+g/2,sq_minY=d,n.beginPath(),n.moveTo(sq_minX,sq_minY),n.lineTo(sq_minX,sq_minY+g),n.lineWidth=10,n.lineTo(sq_minX+g,sq_minY+g),n.lineTo(sq_minX+g,sq_minY),n.closePath(),n.fillStyle=s,n.fill(),n.strokeStyle="#ff0000",n.stroke(),sq_minX=l,sq_minY=d-g,n.beginPath(),n.moveTo(sq_minX,sq_minY),n.lineTo(sq_minX,sq_minY+g),n.lineWidth=10,n.lineTo(sq_minX+g,sq_minY+g),n.lineTo(sq_minX+g,sq_minY),n.closePath(),n.fillStyle=s,n.fill(),n.strokeStyle="#ff0000",n.stroke()}function annotate_triangle(n,t,e,i){var a=10;do_annotation(n,e/2+a,0+a,0-a,t-a,e-a,t-a,"red"),recursive_annotate(n,t/2,e/2,t/2,e/4,i,i)}function recursive_annotate(n,t,e,i,a,o,r){if((o-=1)>0){do_annotation(n,a+e/2,i+t,a+e,i,a,i,"red",o/r,r-o);var s=t/2,l=e/2,_=i-s,d=a+l/2,g=i+s,m=a+e-l/2;recursive_annotate(n,s,l,i+s,a-l/2,o,r),recursive_annotate(n,s,l,_,d,o,r),recursive_annotate(n,s,l,g,m,o,r)}}function do_annotation(n,t,e,i,a,o,r,s,l=1,_=0){var d=l,g=Math.floor(50*d);n.font=`${g}px Math`,n.fillStyle=s,n.fontweight="bold";var m=(t+i)/2-40*l,c=(e+a)/2-5*l;_>0?n.fillText(`d/${Math.pow(2,_)}`,m,c):n.fillText("d",m,c);m=(i+o)/2-5*l,c=(a+r)/2+60*l;_>0?n.fillText(`d/${Math.pow(2,_)}`,m,c):n.fillText("d",m,c);m=(o+t)/2+5*l,c=(r+e)/2-5*l;_>0?n.fillText(`d/${Math.pow(2,_)}`,m,c):n.fillText("d",m,c)}function tripinski(n,t,e,i){draw_triangle(n,e/2,0,0,t,e,t,"#000"),triangle(n,t/2,e/2,t/2,e/4,i)}function triangle(n,t,e,i,a,o){if((o-=1)>0){draw_triangle(n,a,i,a+e,i,a+e/2,i+t,"#fff");var r=t/2,s=e/2,l=i-r,_=a+s/2,d=i+r,g=a+e-s/2;triangle(n,r,s,i+r,a-s/2,o),triangle(n,r,s,l,_,o),triangle(n,r,s,d,g,o)}}function draw_triangle(n,t,e,i,a,o,r,s){n.beginPath(),n.moveTo(t,e),n.lineTo(i,a),n.lineTo(o,r),n.lineTo(t,e),n.closePath(),n.fillStyle=s,n.fill()}var h=600,w=600,drawingCanvas=document.getElementById("full_triangle"),context=drawingCanvas.getContext("2d");tripinski(context,h,w,10),tripinski(context=(drawingCanvas=document.getElementById("triangle_1")).getContext("2d"),500,500,2),annotate_triangle(context,500,500,2),tripinski(context=(drawingCanvas=document.getElementById("triangle_2")).getContext("2d"),500,500,3),annotate_triangle(context,500,500,3),tripinski(context=(drawingCanvas=document.getElementById("triangle_3")).getContext("2d"),500,500,4),annotate_triangle(context,500,500,4),tripinski(context=(drawingCanvas=document.getElementById("areaTriangle_1")).getContext("2d"),500,500,2),tripinski(context=(drawingCanvas=document.getElementById("areaTriangle_2")).getContext("2d"),500,500,3),tripinski(context=(drawingCanvas=document.getElementById("areaTriangle_3")).getContext("2d"),500,500,4),tripinski(context=(drawingCanvas=document.getElementById("square_1")).getContext("2d"),500,500,2),sierpinski_square(context,500,500,2),tripinski(context=(drawingCanvas=document.getElementById("square_2")).getContext("2d"),500,500,3),sierpinski_square(context,500,500,3),tripinski(context=(drawingCanvas=document.getElementById("square_3")).getContext("2d"),500,500,4),sierpinski_square(context,500,500,4);</script>]]></content><author><name>Arjun Karuvally</name></author><category term="math"/><category term="nature"/><summary type="html"><![CDATA[A Glimpse into Infinity!]]></summary></entry><entry><title type="html">Memory and the Energy Paradigm</title><link href="https://arjunkaruvally.github.io/blog/2023/introducingMemory/" rel="alternate" type="text/html" title="Memory and the Energy Paradigm"/><published>2023-09-14T00:00:00+00:00</published><updated>2023-09-14T00:00:00+00:00</updated><id>https://arjunkaruvally.github.io/blog/2023/introducingMemory</id><content type="html" xml:base="https://arjunkaruvally.github.io/blog/2023/introducingMemory/"><![CDATA[<p>Memory modeling is an expansive research field that aims to understand and suggest mechanisms for storing and processing memories in both human brains and artificial neural networks. While the topic of memory is vast and can’t be covered fully in a short piece, I’ll highlight a few foundational paradigms that have shaped memory research, culminating with the energy paradigm that has been particularly influential in recent decades.</p> <p>One might wonder, Why focus on memory? What role does it play in the larger context of intelligence? The importance of memory in understanding intelligence, both in biological and artificial realms, was recognized early on by scientists.</p> <h2 id="biological-intelligence">Biological Intelligence</h2> <p>First, let us look at the question of memory from the perspective of biology. Initial explorations (late 1800s to early 1900s) into memory commenced with pioneering work by psychologists such as William James and Hermann Ebbinghaus. They investigated the structure and function of memory using introspection and experimental psychology. The mid-20th century saw advancements in understanding the neurological basis of memory, largely driven by neuropsychological studies on individuals with brain injuries (e.g., the famous case of HM). During this period, the multi-store model of memory which includes sensory, short-term, and long-term stores, was proposed by Atkinson and Shiffrin (1968). From the 1980s, the advent of neuroimaging technologies like MRI and PET scans facilitated a deeper understanding of the brain’s memory systems and researchers started to unveil the molecular and cellular basis of memory, with Eric Kandel’s work on synaptic plasticity being notably influential. From 1990s, we have seen the rise of computational models to simulate neural processes underlying memory. These models aided in understanding how neural networks in the brain process, store, and retrieve information.</p> <h2 id="artificial-intelligence">Artificial Intelligence</h2> <p>Almost parallel to these developments, the domain of artificial intelligence (AI) also saw memory-centric developments. In the early 1950s, scientists started to take the question of intelligence seriously and introduced the symbolic paradigm. In this paradigm, AI systems were imagined as rule-based system that manipulated symbolic entities to achieve its goals. Here, memory played a crucial role as a fixed data-structure upon which the rule based systems operated. Later, from the 1980s, a new kind of science started taking shape broadly termed the connectionist paradigm. The models of intelligence based upon this paradigm took inspiration from how biological neurons worked and proposed what are termed as artificial neural networks (ANNs). At the point, ANNs were at their infancy, only capable of simple tasks in the purview of small university research labs and people pursuing ANN research were considered heretics.</p> <h2 id="convergence---artificial-neural-networks">Convergence - Artificial Neural Networks</h2> <p>It’s fascinating to note that both biological and artificial memory research, despite their distinct origins, eventually converged on the potential of ANNs. With this convergence came the pressing need to address how memory works in ANNs. At the time, a group of researchers broadly called the behaviourists started investigating memory in neural networks by looking at randomly connected networks. They found some evidence for the ability of memory storage in neural networks but as a major drawback of the approach, the scientific basis for the memory capabilities could not be uncovered just looking at the behavioral properties of networks alone. It was at this time, a radical idea for modeling memory started taking shape pioneered by J. J. Hopfield - a theoretical physicist who was curious enough to dabble in the abstract notions of memory research.</p> <h2 id="the-energy-paradigm">The Energy Paradigm</h2> <p>Hopfield pioneered a mathematically robust model for memory in ANNs using what’s known as the energy paradigm. I believe this paradigm stands tall as one of the most significant milestones in contemporary memory research. Its enduring relevance from the 1980s till now certainly attests to its importance.</p> <p>To truly appreciate the energy paradigm, let’s first understand its primary target: <em>associative memory</em>.</p> <h3 id="associative-memory">Associative Memory</h3> <p>Associative memory lends humans the ability to associative stimuli (visual, auditory, etc) with previous experiences. Imagine you’re in a room with shelves full of books. Each book has a unique cover and a story inside. Now, when you see a book with a cover depicting a beach, your mind might automatically recall the last beach vacation you took, the feel of the sand, the sound of waves, or even the book you were reading while sunbathing. This link between the book’s cover and your personal memories of the beach is a basic example of association. Your brain didn’t look at every single memory to find the beach-related one; it simply “jumped” to it because of the connection or association it had made earlier.</p> <p>Associative memory is all about relationships. Instead of retrieving information by a specific “address” (like pulling out a book from a shelf using a specific order number), the brain retrieves memories based on a web of associations. See a dog? You might remember your childhood pet. Smell a certain perfume? You might recall a person who wore it. In each case, one piece of information (e.g., the smell of a perfume) is linked to another (e.g., a specific person). Associative memory is like the magical threads in our brain that connect various pieces of information. When one piece is activated, it can pull on these threads and bring forward related memories, making our experiences rich and interconnected. It’s one of the many wonders of how our brain works, linking moments, feelings, and knowledge in an intricate web of connections.</p> <h3 id="the-energy-paradigm-for-associative-memory">The energy paradigm for associative memory</h3> <p>The concept of the “energy paradigm” might sound technical, but let’s break it down using simple ideas and analogies.</p> <blockquote> <p><strong><em>The Landscape Analogy</em></strong>: Imagine a vast landscape with hills, valleys, and basins. In this landscape, a ball can be placed anywhere. If you set it down on a hill, it will roll down into a nearby valley or basin. The ball will continue to roll until it finds the lowest point it can rest in. Each of these low points or basins represents a stable memory or state.</p> </blockquote> <p>In the context of associative memories, the landscape is a metaphorical representation of “energy”. The idea is that memories, when activated or recalled, seek a state of minimum energy, much like the ball seeks the lowest point in the basin. When we try to recall something, our brain pushes the memory into the landscape. It might not land directly in a basin (or a perfect recall of the memory). Instead, it might land on a hill. But, just like the ball, it will move to the nearest basin, which represents the memory that our brain thinks is the closest match. This process is why sometimes our recall isn’t perfect. Our memories can get influenced by nearby “basins” or other related memories. It’s also why certain triggers can suddenly bring back forgotten memories. They push our memory into the right basin.</p> <h3 id="why-is-the-energy-paradigm-useful">Why is the Energy Paradigm Useful?</h3> <ol> <li> <p><em>Error Correction</em>: The energy paradigm helps in understanding how our brains can correct distorted or partial information. If you see a part of a familiar object, your brain can often “fill in” the rest, because it’s rolling the memory-ball down to the closest complete memory-basin.</p> </li> <li> <p><em>Robustness</em>: Memories stored this way can resist “noise” or interference. Even if some details are fuzzy, the overall memory can still be recalled.</p> </li> <li> <p><em>Associations</em> (See what I did there): It explains why certain memories can trigger related memories. They’re like basins located close together in the landscape. If the ball rolls near the edge of one basin, it might easily move over to a neighboring one.</p> </li> </ol> <h3 id="wrapping-up">Wrapping Up</h3> <p>The energy paradigm in associative memories provides a way to visualize and understand the complex processes of memory recall and storage. By thinking of memories as seeking the lowest energy or the most stable state, we get insight into how our brains efficiently store, recall, and sometimes even mix up or modify our memories. It’s a reminder of the intricate and dynamic nature of our cognitive processes.</p>]]></content><author><name>Arjun Karuvally</name></author><category term="memory"/><category term="intelligence"/><summary type="html"><![CDATA[A brief historical account of the quest for understanding memory and the energy paradigm]]></summary></entry></feed>