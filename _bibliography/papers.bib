---
---

@string{aps = {American Physical Society,}}

@inproceedings{DBLP:conf/iclr/AhmedKPG19,
  author       = {Zafarali Ahmed and
                  Arjun Karuvally and
                  Doina Precup and
                  Simon Gravel},
  title        = {Learning proposals for sequential importance samplers using reinforced
                  variational inference},
  booktitle    = {Deep Reinforcement Learning Meets Structured Prediction, {ICLR} 2019
                  Workshop, New Orleans, Louisiana, United States, May 6, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=HJgxTf89vV},
  timestamp    = {Thu, 25 Jul 2019 16:26:32 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/AhmedKPG19.bib},
  pdf = {https://openreview.net/pdf?id=HJgxTf89vV},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2212-05563,
  author       = {Arjun Karuvally and
                  Terry J. Sejnowski and
                  Hava T. Siegelmann},
  title        = {Energy-based General Sequential Episodic Memory Networks at the Adiabatic
                  Limit},
  journal      = {CoRR},
  volume       = {abs/2212.05563},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2212.05563},
  doi          = {10.48550/arXiv.2212.05563},
  eprinttype    = {arXiv},
  eprint       = {2212.05563},
  timestamp    = {Mon, 02 Jan 2023 15:09:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-05563.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  arxiv={2212.05563}
}

@inproceedings{DBLP:conf/icml/KaruvallySS23,
  author       = {Arjun Karuvally and
                  Terrence J. Sejnowski and
                  Hava T. Siegelmann},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {General Sequential Episodic Memory Model},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {15900--15910},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/karuvally23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/KaruvallySS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  selected={true},
  preview={gsemm.gif},
  pdf={https://proceedings.mlr.press/v202/karuvally23a/karuvally23a.pdf},
  abstract={The state-of-the-art memory model is the General Associative Memory Model, a generalization of the classical Hopfield network. Like its ancestor, the general associative memory has a well-defined state-dependant energy surface, and its memories correlate with its fixed points. This is unlike human memories, which are commonly sequential rather than separated fixed points. In this paper, we introduce a class of General Sequential Episodic Memory Models (GSEMM) that, in the adiabatic limit, exhibit a dynamic energy surface, leading to a series of meta-stable states capable of encoding memory sequences. A multiple-timescale architecture enables the dynamic nature of the energy surface with newly introduced asymmetric synapses and signal propagation delays. We demonstrate its dense capacity under polynomial activation functions. GSEMM combines separate memories, short and long sequential episodic memories, under a unified theoretical framework, demonstrating how energy-based memory modeling can provide richer, human-like episodes.}
}

@inproceedings{karuvally2023episodic,
  title={Episodic Memory Theory of Recurrent Neural Networks: Insights into Long-Term Information Storage and Manipulation},
  author={Arjun Karuvally and Peter DelMastro and Hava T. Siegelmann},
  year={2023},
  booktitle = {Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML) at the 
  40th International Conference on Machine Learning ICML 2023},
  selected={true},
  pdf={https://openreview.net/pdf?id=PYoEjBFAIM},
  abstract={Recurrent neural networks (RNNs) have emerged as powerful models capable of storing and manipulating external information over long periods in various domains. Yet, the mechanisms that underly this behavior remain a mystery due to the black-box nature of these models. This paper addresses this question by proposing an episodic memory theory of RNN dynamics, enabling a more comprehensive understanding of the RNN weights as memories and inter-memory interactions. This approach sheds light on the inner workings of RNNs and connects to existing research on memory representation and organization. The theory extends the current linearization approaches by providing alternative interpretations of the eigenspectrum and its connection to the long-term storage and manipulation of information. We discuss how the segregation, representation, and composition of the variable binding problem—a fundamental question in cognitive science and artificial intelligence—can be mechanistically interpreted within the theory. Using an elementary task - repeat copy, we demonstrate the validity of the theory in experimental settings. Our work represents a step towards opening the black box of RNNs, offering new insights into their functionality and bridging the gap between recurrent neural networks and memory models.}
}

@inproceedings{Jha2018CacheMR,
  title={Cache Miss Rate Predictability via Neural Networks},
  author={Arjun Karuvally and
          Rishikesh Jha and
          Saket Tiwari and
          J. Eliot B. Moss },
  year={2018},
  booktitle = {Workshop on ML for Systems at NeurIPS 2018},
  url={https://api.semanticscholar.org/CorpusID:202611116},
  pdf={https://mlforsystems.org/assets/papers/neurips2018/cache_jha_2018.pdf}
}
